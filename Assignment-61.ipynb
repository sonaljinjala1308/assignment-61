{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0b8408b-47cf-4295-8a64-bcfb755c2439",
   "metadata": {},
   "source": [
    "### Q1. What is an ensemble technique in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef8869d",
   "metadata": {},
   "source": [
    "Ensemble techniques in machine learning are methods that combine the predictions from multiple individual models (often referred to as \"base models\" or \"weak learners\") to make more accurate and robust predictions than any single model. The idea behind ensemble techniques is to leverage the collective wisdom of multiple models to improve overall predictive performance. Ensembles are a powerful approach in machine learning and can be used for both classification and regression tasks.\n",
    "\n",
    "Here are some key ensemble techniques:\n",
    "\n",
    "#### Bagging (Bootstrap Aggregating): \n",
    "Bagging involves training multiple base models independently on different subsets of the training data, often by randomly sampling with replacement (bootstrap samples). The predictions from these models are then combined, typically by averaging (for regression) or voting (for classification). Random Forest is a well-known ensemble method that uses bagging with decision trees as base models.\n",
    "\n",
    "#### Boosting: \n",
    "Boosting is an iterative ensemble technique that focuses on training multiple base models sequentially, where each model corrects the errors of its predecessor. Boosting methods assign weights to training instances to emphasize the mistakes and downplay the correctly predicted instances. Gradient Boosting and AdaBoost are popular boosting algorithms.\n",
    "\n",
    "#### Stacking (Stacked Generalization): \n",
    "Stacking combines predictions from multiple base models by training a meta-model (also known as a \"blender\" or \"stacking model\") on their outputs. The base models make predictions on the same dataset, and the meta-model learns how to combine these predictions to make the final prediction. Stacking can capture more complex relationships between base models.\n",
    "\n",
    "#### Voting: \n",
    "Voting is a straightforward ensemble technique where multiple base models make predictions, and the final prediction is determined by a majority vote (for classification) or an average (for regression). There are two common types of voting:\n",
    "Hard Voting: Each base model's prediction is counted as a single vote.\n",
    "Soft Voting: Each base model's prediction is assigned a weight, and the final prediction is a weighted average.\n",
    "\n",
    "\n",
    "\n",
    "Ensemble techniques are effective because they can reduce overfitting, increase model generalization, and improve model robustness. They work well when individual base models have different strengths and weaknesses, as the ensemble can leverage their collective knowledge to make better predictions. Ensemble methods are widely used in various machine learning competitions and real-world applications to achieve state-of-the-art results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565c5d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00544e61-8b3e-41d4-a699-e717c455ee3a",
   "metadata": {},
   "source": [
    "### Q2. Why are ensemble techniques used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da6f3b",
   "metadata": {},
   "source": [
    "Ensemble techniques are used in machine learning for several important reasons:\n",
    "\n",
    "#### Improved Predictive Performance: \n",
    "The primary motivation for using ensemble techniques is to improve predictive accuracy. Ensembles combine the predictions from multiple individual models, and as a result, they often achieve higher accuracy compared to single models. This is especially valuable in tasks where high accuracy is crucial, such as image recognition, fraud detection, and medical diagnosis.\n",
    "\n",
    "\n",
    "#### Reduced Overfitting: \n",
    "Ensemble methods can reduce the risk of overfitting, which occurs when a model learns to perform exceptionally well on the training data but doesn't generalize well to unseen data. By combining multiple models with potentially different sources of error, ensembles can mitigate overfitting and produce more robust predictions on new data.\n",
    "\n",
    "\n",
    "#### Model Robustness: \n",
    "Ensembles are more robust to outliers and noisy data. If an individual model is affected by outliers or noise, the ensemble's aggregated prediction is less likely to be influenced by these anomalies.\n",
    "\n",
    "\n",
    "#### Handling Complex Relationships: \n",
    "In cases where the underlying relationship between input features and the target variable is complex or nonlinear, ensembles can capture this complexity more effectively by combining models with different modeling assumptions.\n",
    "\n",
    "\n",
    "#### Model Stability: \n",
    "Ensembles can make model predictions more stable over time. Small changes in the training dataset or the model's hyperparameters may not result in drastic changes in ensemble predictions, making them more dependable in production environments.\n",
    "\n",
    "\n",
    "#### Feature Selection: \n",
    "Some ensemble methods, such as feature importance analysis in Random Forests, can be used to identify the most important features in a dataset. This can aid in feature selection and dimensionality reduction.\n",
    "\n",
    "\n",
    "#### Versatility: \n",
    "Ensemble techniques can be applied to a wide range of machine learning algorithms, including decision trees, support vector machines, neural networks, and more. This versatility allows practitioners to choose the base models that best suit their data and problem domain.\n",
    "\n",
    "\n",
    "#### State-of-the-Art Performance: \n",
    "Ensemble methods have consistently demonstrated their effectiveness in various machine learning competitions and real-world applications. They often yield state-of-the-art results and are considered a best practice in many fields.\n",
    "\n",
    "    \n",
    "Overall, ensemble techniques are a valuable tool in a machine learning practitioner's toolbox because they offer a systematic way to harness the collective knowledge of multiple models, leading to improved model performance and robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4abe06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bc342db-95a7-4296-b604-544da07ded99",
   "metadata": {},
   "source": [
    "### Q3. What is bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c77d0c",
   "metadata": {},
   "source": [
    "Bagging, short for \"Bootstrap Aggregating,\" is an ensemble machine learning technique that aims to improve the accuracy and robustness of predictive models. Bagging achieves this by training multiple instances of a base model (often a decision tree) on different subsets of the training data, and then combining their predictions.\n",
    "\n",
    "Here's how bagging works:\n",
    "\n",
    "#### Bootstrapped Sampling: \n",
    "Bagging begins by creating multiple random subsets of the original training dataset through a process called bootstrapped sampling. This means that for each subset, data points are selected randomly from the original dataset with replacement. As a result, some data points may appear multiple times in a subset, while others may not appear at all.\n",
    "\n",
    "#### Base Model Training: \n",
    "A base model (e.g., a decision tree) is trained independently on each of these bootstrapped subsets. Each base model sees a slightly different version of the data.\n",
    "\n",
    "#### Prediction Aggregation: \n",
    "When making predictions, each base model predicts the target variable for the test data. For classification tasks, the most common prediction aggregation methods are:\n",
    "\n",
    "* Voting: In a classification ensemble, each base model's prediction is considered as a vote for a particular class. The class with the majority of votes becomes the final prediction.\n",
    "\n",
    "* Averaging: In a regression ensemble, each base model's prediction is averaged to produce the final prediction.\n",
    "\n",
    "\n",
    "Bagging is particularly effective when the base model is prone to overfitting, such as deep decision trees. By training multiple trees on different subsets of data and combining their predictions, bagging reduces overfitting and increases the model's ability to generalize to unseen data.\n",
    "\n",
    "One of the most well-known algorithms that uses bagging is the Random Forest algorithm. Random Forests are ensembles of decision trees, where each tree is trained on a bootstrapped subset of the data, and additional randomness is introduced during tree construction to further improve model diversity and accuracy.\n",
    "\n",
    "Bagging can be applied to various base models and is not limited to decision trees. It is a versatile and widely used technique in ensemble learning, known for its ability to improve predictive performance and model robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3736c22e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43e6536f-51fe-4080-8b97-ea8fda00f657",
   "metadata": {},
   "source": [
    "### Q4. What is boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d740060",
   "metadata": {},
   "source": [
    "Boosting is an ensemble machine learning technique that aims to improve the accuracy of predictive models by combining multiple weak learners (base models) in a sequential and adaptive manner. Unlike bagging, which trains base models independently, boosting focuses on training them sequentially, where each subsequent model tries to correct the errors made by the previous ones. The key idea is to give more weight to the training instances that are misclassified by earlier models, thereby emphasizing the \"hard-to-learn\" examples.\n",
    "\n",
    "Here's how boosting typically works:\n",
    "\n",
    "#### Initialization:\n",
    "Boosting starts with an initial model, often a simple one, like a single decision stump (a shallow decision tree with one split). All training instances are given equal weights.\n",
    "\n",
    "#### Sequential Training: \n",
    "Boosting trains multiple base models sequentially. During each iteration (also called a \"round\"), a new base model is trained on the training data, and its performance is evaluated.\n",
    "\n",
    "#### Instance Weighting: \n",
    "After each iteration, the weights of the training instances are adjusted. Misclassified instances are assigned higher weights to make them more influential in the subsequent rounds, while correctly classified instances have their weights reduced.\n",
    "\n",
    "#### Combining Predictions:\n",
    "Predictions from each base model are combined using a weighted average. In classification tasks, the models may also vote, with their votes weighted by their performance.\n",
    "\n",
    "#### Iterative Process: \n",
    "The boosting process continues for a predetermined number of rounds or until a stopping criterion is met. The final prediction is obtained by aggregating the predictions of all base models.\n",
    "\n",
    "#### Popular boosting algorithms include:\n",
    "\n",
    "* AdaBoost (Adaptive Boosting): AdaBoost is one of the earliest and most well-known boosting algorithms. It assigns higher    \n",
    "  weights to misclassified instances and focuses on improving the classification accuracy.\n",
    "\n",
    "* Gradient Boosting: Gradient Boosting, which includes algorithms like Gradient Boosting Machines (GBM) and XGBoost, builds base \n",
    "  models sequentially by fitting them to the residuals (errors) of previous models. This approach is particularly effective for \n",
    "  regression tasks.\n",
    "\n",
    "* LightGBM and CatBoost:These are variations of gradient boosting algorithms designed for better performance and efficiency,   \n",
    "  often used in competitions and real-world applications.\n",
    "\n",
    "Boosting is favored for its ability to improve the performance of weak learners, making it suitable for a wide range of tasks. It is robust, adaptive, and capable of capturing complex patterns in data. However, boosting is sensitive to noisy data and outliers and may be prone to overfitting if not appropriately tuned or if too many base models are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762b398c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c60e21b9-a2ee-4005-abaf-bac766025628",
   "metadata": {},
   "source": [
    "### Q5. What are the benefits of using ensemble techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507d68a6",
   "metadata": {},
   "source": [
    "Ensemble techniques offer several significant benefits in machine learning, making them a valuable tool for improving predictive models\n",
    "and addressing various challenges:\n",
    "\n",
    "* #### Improved Predictive Performance: \n",
    "Ensembles typically achieve higher accuracy and better generalization compared to individual models. By combining multiple models, ensembles can capture different patterns in the data, reducing bias and variance and leading to more robust predictions.\n",
    "\n",
    "* #### Reduction in Overfitting: \n",
    "Ensembles help mitigate overfitting, which occurs when a model learns to perform exceptionally well on the training data but fails to generalize to unseen data. The averaging or voting process in ensembles often smooths out the noise and reduces the impact of outliers, making models more robust.\n",
    "\n",
    "* #### Enhanced Robustness: \n",
    "Ensembles are more robust to noisy data and outliers. If a single model is adversely affected by outliers or noisy instances, the ensemble's aggregated prediction is less likely to be influenced by these anomalies.\n",
    "\n",
    "* #### Effective Handling of Complex Relationships: \n",
    "Ensembles can capture complex and nonlinear relationships between input features and the target variable more effectively by combining models with different modeling assumptions. This is valuable in tasks where the relationship is intricate and challenging to model with a single approach.\n",
    "\n",
    "* #### Model Selection and Feature Importance: \n",
    "Some ensemble methods, such as Random Forest, provide insights into feature importance. They can help identify the most relevant features in a dataset, aiding in feature selection and dimensionality reduction.\n",
    "\n",
    "* #### State-of-the-Art Performance:\n",
    "Ensembles have consistently demonstrated their effectiveness in machine learning competitions and real-world applications. They often yield state-of-the-art results and are considered a best practice for improving model performance.\n",
    "\n",
    "* #### Versatility: \n",
    "Ensemble techniques can be applied to a wide range of machine learning algorithms, including decision trees, support vector machines, neural networks, and more. This versatility allows practitioners to choose the base models that best suit their data and problem domain.\n",
    "\n",
    "* #### Model Stability:\n",
    "Ensembles can make model predictions more stable over time. Small changes in the training dataset or the model's hyperparameters may not result in drastic changes in ensemble predictions, making them more dependable in production environments.\n",
    "\n",
    "* #### Adaptive Learning: \n",
    "Boosting, a type of ensemble technique, adapts to the data over iterations by focusing on examples that are challenging to classify. This adaptive learning process can lead to improved model performance.\n",
    "\n",
    "* #### Reduced Bias: \n",
    "Ensembles often reduce bias, as individual models may have different sources of error. By combining their predictions, ensembles can achieve a more balanced and accurate result.\n",
    "\n",
    "    \n",
    "In summary, ensemble techniques are widely used in machine learning because they harness the collective knowledge of multiple models, leading \n",
    "to improved predictive performance, model robustness, and generalization. They are particularly valuable when individual models have different\n",
    "strengths and weaknesses or when dealing with complex and noisy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8d7943",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc92ac3d-df47-42c6-b91a-346c394f6533",
   "metadata": {},
   "source": [
    "### Q6. Are ensemble techniques always better than individual models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecc27e0",
   "metadata": {},
   "source": [
    "Ensemble techniques are powerful tools in machine learning, but they are not always guaranteed to be better than individual models.\n",
    "Whether ensembles are superior depends on several factors and the specific problem at hand. Here are some considerations:\n",
    "\n",
    "* #### Quality of Base Models: \n",
    "The effectiveness of an ensemble depends on the quality of the base models it combines. If the individual models are already highly accurate and robust, combining them may provide only marginal improvements or even lead to overfitting.\n",
    "\n",
    "* #### Diversity of Base Models: \n",
    "Ensembles benefit from diverse base models that make different types of errors. If the base models are too similar or have similar sources of error, the ensemble may not perform significantly better than an individual model.\n",
    "\n",
    "* #### Amount of Data: \n",
    "For small datasets, ensembles are more likely to outperform individual models because they can compensate for the limited amount of training data by combining multiple sources of information. However, on very large datasets, a single powerful model may be sufficient.\n",
    "\n",
    "* #### Data Quality: \n",
    "If the data is noisy, contains outliers, or has missing values, ensembles are more likely to handle these challenges effectively, making them a better choice.\n",
    "\n",
    "* #### Model Complexity: \n",
    "Ensembles can be computationally expensive, especially if they involve training numerous base models. For some applications with strict latency requirements, using a single model may be more practical.\n",
    "\n",
    "* #### Interpretability: \n",
    "Individual models are often easier to interpret and explain than ensembles. If model interpretability is a critical requirement, an ensemble may not be the best choice.\n",
    "\n",
    "* #### Overfitting Risk: \n",
    "Ensembles can reduce overfitting, but they can also overfit if not properly regularized or if too many base models are included. Balancing the number of base models and their complexity is essential.\n",
    "\n",
    "* #### Training Time: \n",
    "Ensembles typically require more time to train because they involve multiple models. If training time is a significant concern, an individual model may be preferred.\n",
    "\n",
    "* #### Problem Complexity: \n",
    "For simple and well-structured problems, a single model may suffice. Ensembles tend to shine in complex and challenging problem domains where the relationships between features and the target variable are intricate.\n",
    "\n",
    "In practice, the choice between using an ensemble or an individual model depends on a careful analysis of the problem, the data, and \n",
    "computational resources. It's common to start with a single model and then experiment with ensembles to see if they lead to improvements. \n",
    "Ensembles are a valuable tool, but they should be used judiciously and not as a one-size-fits-all solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce38d23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd2c9d7a-5e69-4e80-bfbb-5081648cddf9",
   "metadata": {},
   "source": [
    "### Q7. How is the confidence interval calculated using bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d172462d",
   "metadata": {},
   "source": [
    "A confidence interval (CI) can be calculated using bootstrap resampling as follows:\n",
    "\n",
    "* Collect Your Data: \n",
    "    Start with your original dataset, which contains your observed data points.\n",
    "\n",
    "* Choose the Desired Confidence Level: Decide on the confidence level you want for your CI. Common choices are 90%, 95%, or 99% confidence.\n",
    "\n",
    "* Resample with Replacement: \n",
    "    Perform the following steps many times (typically thousands of times):\n",
    "\n",
    "    * Randomly select n data points from your original dataset with replacement, where n is the size of your dataset. This new         dataset is called a \"bootstrap sample.\"\n",
    "    * Calculate the statistic of interest (e.g., mean, median, standard deviation, etc.) on the bootstrap sample. This gives you       one resampled statistic.\n",
    "\n",
    "* Create a Sampling Distribution: \n",
    "    After performing the resampling process a large number of times, you'll have a distribution of the resampled statistics. This distribution\n",
    "    is known as the \"sampling distribution\" of the statistic.\n",
    "\n",
    "* Calculate the Confidence Interval: \n",
    "    To create a confidence interval, you need to find the values in the sampling distribution that correspond to your chosen confidence level. \n",
    "    For example, for a 95% confidence interval, you would typically find the 2.5th percentile and the 97.5th percentile of the sampling\n",
    "    distribution.\n",
    "\n",
    "* Report the Confidence Interval: \n",
    "    The calculated percentiles from the sampling distribution represent the lower and upper bounds of your confidence interval. You can report \n",
    "    these values along with the statistic of interest to convey the range within which you are confident the true population parameter lies.\n",
    "    \n",
    "Here's a Python example using NumPy to calculate a bootstrap confidence interval for the mean of a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3887f3d3-597a-40cd-aeaa-bdc5154fdc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap 95% Confidence Interval for Mean: [4.02 7.69]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# dataset\n",
    "data = np.array([1.2, 2.4, 3.5, 4.1, 5.2, 6.3, 7.0, 8.4, 9.7, 10.5])\n",
    "\n",
    "# Number of bootstrap samples to generate\n",
    "num_samples = 10000\n",
    "\n",
    "# Bootstrap resampling and calculation of means\n",
    "bootstrap_means = []\n",
    "for _ in range(num_samples):\n",
    "    # Generate a bootstrap sample by sampling with replacement\n",
    "    bootstrap_sample = np.random.choice(data, size=len(data), replace=True)\n",
    "    \n",
    "    # Calculate the mean of the bootstrap sample\n",
    "    bootstrap_mean = np.mean(bootstrap_sample)\n",
    "    bootstrap_means.append(bootstrap_mean)\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "\n",
    "print(\"Bootstrap 95% Confidence Interval for Mean:\", confidence_interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45fb448-3e96-47ff-9fe2-58c0ecc8f786",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This code generates a bootstrap confidence interval for the mean of the dataset by resampling and calculating means from the resampled data. \n",
    "The confidence_interval variable contains the lower and upper bounds of the 95% confidence interval for the mean.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cb6a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5ad2d71-330c-4199-afed-0f37ba3ed1ac",
   "metadata": {},
   "source": [
    "### Q8. How does bootstrap work and What are the steps involved in bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25441de",
   "metadata": {},
   "source": [
    "Bootstrap is a resampling technique used in statistics and machine learning to estimate the sampling distribution of a statistic or to assess\n",
    "the uncertainty of a parameter estimate. It works by repeatedly resampling (with replacement) from the observed data to create multiple new \n",
    "datasets, from which statistics or parameter estimates are calculated. The steps involved in bootstrap are as follows:\n",
    "\n",
    "#### Collect Your Data: \n",
    "Start with your original dataset, which contains your observed data points. This dataset is often referred to as the \"population\" or \"parent population.\"\n",
    "\n",
    "#### Choose the Number of Resamples: \n",
    "Determine how many bootstrap resamples you want to create. Common choices are 1,000 or 10,000 resamples, but the number can vary based on the desired precision of your estimates.\n",
    "\n",
    "#### Resample with Replacement: \n",
    "For each resample, randomly select data points from your original dataset with replacement. This means that you select a data point, record its value, and then put it back into the dataset before selecting the next one. As a result, some data points may appear multiple times in a resample, while others may not appear at all. Each resample is of the same size as the original dataset.\n",
    "\n",
    "#### Calculate the Statistic of Interest: \n",
    "On each bootstrap resample, calculate the statistic of interest (e.g., mean, median, standard deviation, etc.). This gives you one resampled statistic. This step mimics the process of calculating the statistic on a new random sample from the population.\n",
    "\n",
    "#### Repeat the Process: \n",
    "Repeat steps 3 and 4 for the specified number of resamples (e.g., 1,000 times). Each time, you generate a new dataset by resampling with replacement and calculate the statistic of interest on that dataset.\n",
    "\n",
    "#### Analyze the Resampled Statistics: \n",
    "* After creating a large number of resampled statistics, you can use them to:\n",
    "\n",
    "  * Estimate the sampling distribution of the statistic, which can be used to construct confidence intervals or assess the           variability of the statistic.\n",
    "  * Make inferences about the population parameter or test hypotheses. For example, you can calculate the standard error             of the statistic, perform hypothesis tests, or estimate confidence intervals for population parameters.\n",
    "\n",
    "#### Report Results: \n",
    "You can report various results based on the resampled statistics, such as confidence intervals, hypothesis test p-values, or summary statistics of the parameter of interest.\n",
    "\n",
    "\n",
    "Bootstrap is a powerful and versatile technique that can be applied to various statistical problems. It provides a non-parametric and \n",
    "data-driven approach to understanding the variability and uncertainty associated with sample statistics or parameter estimates, making it \n",
    "particularly useful in cases where analytical methods are not straightforward or assumptions are violated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262f3d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69e68324-8966-4ff0-85a0-325ed26723fa",
   "metadata": {},
   "source": [
    "###  Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a sample of 50 trees and obtain a mean  height of 15 meters and a standard deviation of 2 meters. Use bootstrap to estimate the 95% confidence interval for the population mean height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b957afb8-2e06-4a43-b08e-2cb791145cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap 95% Confidence Interval for Population Mean Height: [14.4494495  15.54934156]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "sample_mean = 15  # Sample mean height\n",
    "sample_stddev = 2  # Sample standard deviation\n",
    "sample_size = 50  # Sample size\n",
    "num_bootstrap_samples = 10000  # Number of bootstrap samples\n",
    "\n",
    "# Create an array to store bootstrap means\n",
    "bootstrap_means = []\n",
    "\n",
    "# Perform bootstrap resampling\n",
    "for _ in range(num_bootstrap_samples):\n",
    "    # Generate a bootstrap sample by resampling with replacement\n",
    "    bootstrap_sample = np.random.normal(sample_mean, sample_stddev, sample_size)\n",
    "    \n",
    "    # Calculate the mean of the bootstrap sample\n",
    "    bootstrap_mean = np.mean(bootstrap_sample)\n",
    "    \n",
    "    # Store the bootstrap mean\n",
    "    bootstrap_means.append(bootstrap_mean)\n",
    "\n",
    "# Calculate the 95% confidence interval\n",
    "confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "\n",
    "print(\"Bootstrap 95% Confidence Interval for Population Mean Height:\", confidence_interval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fbd48c-b343-4b14-b12d-a927114a6f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
